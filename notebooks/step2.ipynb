{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 116/116 [00:01<00:00, 103.43it/s]\n"
     ]
    }
   ],
   "source": [
    "cat_feats = train.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "for feat in tqdm(cat_feats):\n",
    "    train[feat + '_id'] = pd.factorize( train[feat] )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_feats = [feat for feat in train.columns if 'cont' in feat]\n",
    "id_feats  = [feat for feat in train.columns if '_id' in feat]\n",
    "\n",
    "X = train[num_feats + id_feats].values\n",
    "y = train['loss'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dt 1362.8732376665685 1.7448180097867887\n",
      "rf 1317.5370763126657 2.540738629807585\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    ('dt', DecisionTreeRegressor(max_depth=10)), \n",
    "    ('rf', RandomForestRegressor(max_depth=10, n_estimators=20)),\n",
    "]\n",
    "\n",
    "kf = KFold(n_splits=3, shuffle = True, random_state = 2018)\n",
    "\n",
    "for model_name, model in models:\n",
    "    scores = []\n",
    "    for train_idx, test_idx in kf.split(X):\n",
    "        model.fit(X[train_idx], y[train_idx])\n",
    "        y_pred = model.predict(X[test_idx])\n",
    "        \n",
    "        score = mean_absolute_error(y[test_idx], y_pred)\n",
    "        scores.append(score)\n",
    "        \n",
    "    print(model_name, np.mean(scores), np.std(scores) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = xgb.XGBRegressor(\n",
    "    max_depth = 12,\n",
    "    learning_rate = 0.2,\n",
    "    n_estimators = 20,\n",
    "    objective = 'reg:linear',\n",
    "    nthread = -1,\n",
    "    subsample = 0.7,\n",
    "    colsample_bytree = 0.6,\n",
    "    seed = 2018\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mae:2456.46\tvalidation_1-mae:2426.53\n",
      "[19]\tvalidation_0-mae:995.507\tvalidation_1-mae:1214.37\n",
      "Fold1, score=1214.3632879701288\n",
      "[0]\tvalidation_0-mae:2439.77\tvalidation_1-mae:2464.2\n",
      "[19]\tvalidation_0-mae:971.745\tvalidation_1-mae:1233.69\n",
      "Fold2, score=1233.6858121375053\n",
      "[0]\tvalidation_0-mae:2443.05\tvalidation_1-mae:2457.38\n",
      "[19]\tvalidation_0-mae:976.681\tvalidation_1-mae:1230.65\n",
      "Fold3, score=1230.6541292733866\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=3, shuffle = True, random_state = 2018)\n",
    "scores = []\n",
    "for num_iter, (train_idx, test_idx) in enumerate(kf.split(X)):\n",
    "    \n",
    "    model.fit(X[train_idx], y[train_idx],\n",
    "       eval_metric='mae',\n",
    "       eval_set=[(X[train_idx], y[train_idx]), (X[test_idx], y[test_idx])],\n",
    "       verbose=20)\n",
    "    \n",
    "    y_pred = model.predict(X[test_idx])\n",
    "    y_pred[y_pred<0] = 0\n",
    "\n",
    "    score = mean_absolute_error(y[test_idx], y_pred)\n",
    "    print(\"Fold{0}, score={1}\".format(num_iter+1, score))\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Log transformation\n",
    "\n",
    "Let's use log transformation for target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dt 1306.4424390640017 7.353250529878342\n",
      "rf 1263.2329451162102 6.530534772533145\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    ('dt', DecisionTreeRegressor(max_depth=10)), \n",
    "    ('rf', RandomForestRegressor(max_depth=10, n_estimators=20)),\n",
    "]\n",
    "\n",
    "kf = KFold(n_splits=3, shuffle = True, random_state = 2018)\n",
    "\n",
    "offset = 0\n",
    "y_log = np.log(y + offset)\n",
    "for model_name, model in models:\n",
    "    scores = []\n",
    "    for train_idx, test_idx in kf.split(X):\n",
    "        model.fit(X[train_idx], y_log[train_idx])\n",
    "        y_pred_log = model.predict(X[test_idx])\n",
    "        y_pred = np.exp(y_pred_log) - offset\n",
    "        \n",
    "        score = mean_absolute_error(y[test_idx], y_pred)\n",
    "        scores.append(score)\n",
    "        \n",
    "    print(model_name, np.mean(scores), np.std(scores) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost + log(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = xgb.XGBRegressor(\n",
    "    max_depth = 12,\n",
    "    learning_rate = 0.3,\n",
    "    n_estimators = 20,\n",
    "    objective = 'reg:linear',\n",
    "    nthread = -1,\n",
    "    subsample = 0.7,\n",
    "    colsample_bytree = 0.6,\n",
    "    seed = 2018\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mae:5.75178\tvalidation_1-mae:5.7428\n",
      "[5]\tvalidation_0-mae:1.88924\tvalidation_1-mae:1.88523\n",
      "[10]\tvalidation_0-mae:0.693799\tvalidation_1-mae:0.706503\n",
      "[15]\tvalidation_0-mae:0.437333\tvalidation_1-mae:0.472321\n",
      "[19]\tvalidation_0-mae:0.392509\tvalidation_1-mae:0.439671\n",
      "Fold1, score=1214.0610349104584\n",
      "[0]\tvalidation_0-mae:5.74742\tvalidation_1-mae:5.75233\n",
      "[5]\tvalidation_0-mae:1.88847\tvalidation_1-mae:1.89159\n",
      "[10]\tvalidation_0-mae:0.692816\tvalidation_1-mae:0.708938\n",
      "[15]\tvalidation_0-mae:0.436444\tvalidation_1-mae:0.473941\n",
      "[19]\tvalidation_0-mae:0.391495\tvalidation_1-mae:0.441838\n",
      "Fold2, score=1237.98094127552\n",
      "[0]\tvalidation_0-mae:5.74817\tvalidation_1-mae:5.75204\n",
      "[5]\tvalidation_0-mae:1.88895\tvalidation_1-mae:1.89127\n",
      "[10]\tvalidation_0-mae:0.694181\tvalidation_1-mae:0.709357\n",
      "[15]\tvalidation_0-mae:0.437371\tvalidation_1-mae:0.47301\n",
      "[19]\tvalidation_0-mae:0.393367\tvalidation_1-mae:0.440859\n",
      "Fold3, score=1231.2871999216247\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1227.7763920358677, 10.075872472937519)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kf = KFold(n_splits=3, shuffle = True, random_state = 2018)\n",
    "scores = []\n",
    "\n",
    "y_log = np.log(y)\n",
    "for num_iter, (train_idx, test_idx) in enumerate(kf.split(X)):\n",
    "    model.fit(X[train_idx], y_log[train_idx],\n",
    "       eval_metric='mae',\n",
    "       eval_set=[(X[train_idx], y_log[train_idx]), (X[test_idx], y_log[test_idx])],\n",
    "       verbose=5)\n",
    "    \n",
    "    y_pred_log = model.predict(X[test_idx])\n",
    "    y_pred = np.exp( y_pred_log )\n",
    "    y_pred[y_pred<0] = 0\n",
    "\n",
    "    score = mean_absolute_error(y[test_idx], y_pred)\n",
    "    print(\"Fold{0}, score={1}\".format(num_iter+1, score))\n",
    "    scores.append(score)\n",
    "    \n",
    "np.mean(scores), np.std(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tcv_agg's l1: 1546.51 + 7.70168\n",
      "[20]\tcv_agg's l1: 1373.26 + 9.42364\n",
      "[30]\tcv_agg's l1: 1324.17 + 8.54692\n",
      "[40]\tcv_agg's l1: 1300.88 + 8.25568\n",
      "[50]\tcv_agg's l1: 1285.73 + 7.70553\n",
      "[60]\tcv_agg's l1: 1277.55 + 6.91272\n",
      "[70]\tcv_agg's l1: 1273.5 + 6.30497\n",
      "[80]\tcv_agg's l1: 1270.39 + 5.28638\n",
      "[90]\tcv_agg's l1: 1268.54 + 5.12904\n",
      "[100]\tcv_agg's l1: 1269.28 + 5.34698\n"
     ]
    }
   ],
   "source": [
    "lgb_params = {'learning_rate'    : 0.35,\n",
    "              'boosting'         : 'gbdt',\n",
    "              'objective'        : 'regression_l1',\n",
    "              'metric'           : 'mae',\n",
    "              'max_depth'        : 12,\n",
    "              'feature_fraction' : 0.9,\n",
    "              'bagging_fraction' : 0.75,\n",
    "              'num_leaves'       : 31,\n",
    "              'bagging_freq'     : 1,\n",
    "              'min_data_per_leaf': 250}\n",
    "\n",
    "\n",
    "lgb_train = lgb.Dataset(data=train[num_feats + id_feats], label=y)\n",
    "\n",
    "cv = lgb.cv(lgb_params, \n",
    "              lgb_train, \n",
    "              num_boost_round=100, \n",
    "              early_stopping_rounds=15,\n",
    "              stratified=False, \n",
    "              verbose_eval=10,\n",
    "              nfold=3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Advanced Objective Function: [link](https://www.kaggle.com/c/allstate-claims-severity/discussion/24520)\n",
    "\n",
    "![](../images/ln_cosh.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
